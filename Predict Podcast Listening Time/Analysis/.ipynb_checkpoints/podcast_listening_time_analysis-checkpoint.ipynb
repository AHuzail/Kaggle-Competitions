{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25593075",
   "metadata": {},
   "source": [
    "# Podcast Listening Time Prediction - Exploratory Data Analysis\n",
    "\n",
    "This notebook explores the Podcast Listening Time Prediction dataset from the Kaggle competition. The goal is to predict how long listeners will tune in to a podcast episode based on various features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e00448",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c947a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-6.0.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /root/tf-env/lib/python3.12/site-packages (from plotly) (1.32.0)\n",
      "Requirement already satisfied: packaging in /root/tf-env/lib/python3.12/site-packages (from plotly) (24.2)\n",
      "Downloading plotly-6.0.1-py3-none-any.whl (14.8 MB)\n",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/14.8 MB\u001b[0m \u001b[31m23.2 kB/s\u001b[0m eta \u001b[36m0:09:20\u001b[0m^C\n",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/14.8 MB\u001b[0m \u001b[31m23.2 kB/s\u001b[0m eta \u001b[36m0:09:20\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d253c13b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# For visualization\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexpress\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpx\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_objects\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgo\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msubplots\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_subplots\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# For visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# For modeling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set visualization style\n",
    "sns.set(style='whitegrid')\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa659a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_path = '../Datasets/train.csv'\n",
    "test_path = '../Datasets/test.csv'\n",
    "sample_submission_path = '../Datasets/sample_submission.csv'\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Training set shape: {train.shape}\")\n",
    "print(f\"Test set shape: {test.shape}\")\n",
    "print(f\"Sample submission shape: {sample_submission.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c0dcec",
   "metadata": {},
   "source": [
    "## 2. Data Overview and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81298cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows of training data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae9378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows of test data\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c5a889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and basic statistics\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e4aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numerical features\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd815c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any duplicate rows in train\n",
    "print(f\"Number of duplicate rows in train: {train.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01fa683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any duplicate rows in test\n",
    "print(f\"Number of duplicate rows in test: {test.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b30cb",
   "metadata": {},
   "source": [
    "## 3. Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ecf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in training set\n",
    "missing_train = train.isnull().sum().sort_values(ascending=False)\n",
    "missing_train_percent = (train.isnull().sum() / train.shape[0] * 100).sort_values(ascending=False)\n",
    "missing_train_df = pd.concat([missing_train, missing_train_percent], axis=1, keys=['Total', 'Percent'])\n",
    "print(\"Missing values in training set:\")\n",
    "missing_train_df[missing_train_df['Total'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a45fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in test set\n",
    "missing_test = test.isnull().sum().sort_values(ascending=False)\n",
    "missing_test_percent = (test.isnull().sum() / test.shape[0] * 100).sort_values(ascending=False)\n",
    "missing_test_df = pd.concat([missing_test, missing_test_percent], axis=1, keys=['Total', 'Percent'])\n",
    "print(\"Missing values in test set:\")\n",
    "missing_test_df[missing_test_df['Total'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262134dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Percentage of Missing Values by Feature')\n",
    "sns.barplot(x=missing_train_df[missing_train_df['Total'] > 0].index, \n",
    "            y=missing_train_df[missing_train_df['Total'] > 0]['Percent'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e51a77a",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ab6d31",
   "metadata": {},
   "source": [
    "### 4.1 Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the distribution of the target variable\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(train['Listening_Time_minutes'], kde=True)\n",
    "plt.title('Distribution of Listening Time')\n",
    "plt.xlabel('Listening Time (minutes)')\n",
    "\n",
    "# Box plot\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=train['Listening_Time_minutes'])\n",
    "plt.title('Box Plot of Listening Time')\n",
    "plt.ylabel('Listening Time (minutes)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Basic statistics of target variable\n",
    "print(train['Listening_Time_minutes'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2513235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any outliers in the target variable\n",
    "Q1 = train['Listening_Time_minutes'].quantile(0.25)\n",
    "Q3 = train['Listening_Time_minutes'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = train[(train['Listening_Time_minutes'] < lower_bound) | \n",
    "                 (train['Listening_Time_minutes'] > upper_bound)]\n",
    "\n",
    "print(f\"Number of outliers in target variable: {len(outliers)}\")\n",
    "print(f\"Percentage of outliers: {len(outliers) / len(train) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd8a55e",
   "metadata": {},
   "source": [
    "### 4.2 Feature Analysis - Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e3cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = [col for col in train.columns if train[col].dtype == 'object']\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Count unique values in each categorical column\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col} - {train[col].nunique()} unique values:\")\n",
    "    print(train[col].value_counts().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db03fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize relationship between categorical features and target variable\n",
    "for col in ['Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment']:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Box plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(x=col, y='Listening_Time_minutes', data=train)\n",
    "    plt.title(f'Listening Time by {col}')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Bar plot for average listening time\n",
    "    plt.subplot(1, 2, 2)\n",
    "    train.groupby(col)['Listening_Time_minutes'].mean().sort_values(ascending=False).plot(kind='bar')\n",
    "    plt.title(f'Average Listening Time by {col}')\n",
    "    plt.ylabel('Average Listening Time (minutes)')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1919806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plots for categorical variables\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i, col in enumerate(['Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment']):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.countplot(y=col, data=train, order=train[col].value_counts().index)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8e2906",
   "metadata": {},
   "source": [
    "### 4.3 Feature Analysis - Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c796dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns (excluding id and target)\n",
    "numerical_cols = [col for col in train.columns if train[col].dtype != 'object' \n",
    "                  and col not in ['id', 'Listening_Time_minutes']]\n",
    "print(f\"Numerical columns: {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f1af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numerical features\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.histplot(train[col].dropna(), kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a20d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots of numerical features vs target\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.scatter(train[col], train['Listening_Time_minutes'], alpha=0.1)\n",
    "    plt.title(f'{col} vs Listening Time')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Listening Time (minutes)')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a234b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between numerical features and target\n",
    "numerical_data = train[numerical_cols + ['Listening_Time_minutes']].copy()\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = numerical_data.corr()\n",
    "print(\"Correlation with target variable:\")\n",
    "print(correlation_matrix['Listening_Time_minutes'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a1d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c4f09e",
   "metadata": {},
   "source": [
    "### 4.4 Relationship Analysis - Episode Length vs Listening Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fb349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationship between episode length and listening time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(train['Episode_Length_minutes'], train['Listening_Time_minutes'], alpha=0.1)\n",
    "plt.title('Episode Length vs Listening Time')\n",
    "plt.xlabel('Episode Length (minutes)')\n",
    "plt.ylabel('Listening Time (minutes)')\n",
    "\n",
    "# Add a line where x=y (perfect retention)\n",
    "max_val = max(train['Episode_Length_minutes'].max(), train['Listening_Time_minutes'].max())\n",
    "plt.plot([0, max_val], [0, max_val], 'r--', label='Perfect Retention')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af892f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate retention rate (listening time / episode length)\n",
    "# Only for rows where episode length is not missing\n",
    "train_with_length = train.dropna(subset=['Episode_Length_minutes']).copy()\n",
    "train_with_length['Retention_Rate'] = train_with_length['Listening_Time_minutes'] / train_with_length['Episode_Length_minutes']\n",
    "\n",
    "# Analyze retention rate distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train_with_length['Retention_Rate'], bins=50, kde=True)\n",
    "plt.title('Distribution of Retention Rate')\n",
    "plt.xlabel('Retention Rate (Listening Time / Episode Length)')\n",
    "plt.axvline(1, color='r', linestyle='--', label='Perfect Retention')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(train_with_length['Retention_Rate'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546583ec",
   "metadata": {},
   "source": [
    "### 4.5 Advanced Analysis - Multivariate Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1785a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between Genre, Episode Length, and Listening Time\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Genre', y='Listening_Time_minutes', data=train)\n",
    "plt.title('Listening Time by Genre')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6966858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listening time by host popularity segments\n",
    "train['Host_Popularity_Segment'] = pd.qcut(train['Host_Popularity_percentage'], 4, labels=['Low', 'Medium-Low', 'Medium-High', 'High'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Host_Popularity_Segment', y='Listening_Time_minutes', data=train)\n",
    "plt.title('Listening Time by Host Popularity Segment')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e3291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how sentiment affects listening time across different genres\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Genre', y='Listening_Time_minutes', hue='Episode_Sentiment', data=train)\n",
    "plt.title('Listening Time by Genre and Sentiment')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Sentiment')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3e108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze effect of number of ads on listening time\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Number_of_Ads', y='Listening_Time_minutes', data=train)\n",
    "plt.title('Listening Time by Number of Ads')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca05975",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c99b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for feature engineering to apply to both train and test\n",
    "def engineer_features(df):\n",
    "    # Create a copy to avoid changing the original dataframe\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Extract episode number from Episode_Title\n",
    "    df_new['Episode_Number'] = df_new['Episode_Title'].str.extract(r'Episode (\\d+)').astype(float)\n",
    "    \n",
    "    # Day of week encoding (numerical)\n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    df_new['Day_Num'] = df_new['Publication_Day'].map({day: i for i, day in enumerate(day_order)})\n",
    "    \n",
    "    # Is weekend feature\n",
    "    df_new['Is_Weekend'] = df_new['Publication_Day'].isin(['Saturday', 'Sunday']).astype(int)\n",
    "    \n",
    "    # Time of day encoding\n",
    "    time_order = ['Morning', 'Afternoon', 'Evening', 'Night']\n",
    "    df_new['Time_Num'] = df_new['Publication_Time'].map({time: i for i, time in enumerate(time_order)})\n",
    "    \n",
    "    # For rows where Episode_Length_minutes is available, calculate proportion of listening time\n",
    "    # This feature will have NaN for rows where Episode_Length_minutes is missing\n",
    "    if 'Listening_Time_minutes' in df_new.columns and 'Episode_Length_minutes' in df_new.columns:\n",
    "        df_new['Retention_Rate'] = df_new['Listening_Time_minutes'] / df_new['Episode_Length_minutes']\n",
    "    \n",
    "    # Create podcast popularity rank features\n",
    "    podcast_avg_listening = df.groupby('Podcast_Name')['Listening_Time_minutes'].mean().reset_index() if 'Listening_Time_minutes' in df.columns else None\n",
    "    \n",
    "    if podcast_avg_listening is not None:\n",
    "        podcast_avg_listening.columns = ['Podcast_Name', 'Avg_Podcast_Listening']\n",
    "        df_new = df_new.merge(podcast_avg_listening, on='Podcast_Name', how='left')\n",
    "    \n",
    "    # Episode Sentiment encoding\n",
    "    sentiment_map = {'Positive': 1, 'Neutral': 0, 'Negative': -1}\n",
    "    df_new['Sentiment_Score'] = df_new['Episode_Sentiment'].map(sentiment_map)\n",
    "    \n",
    "    # Interaction features\n",
    "    df_new['Host_Guest_Popularity_Diff'] = df_new['Host_Popularity_percentage'] - df_new['Guest_Popularity_percentage']\n",
    "    df_new['Host_Guest_Popularity_Product'] = df_new['Host_Popularity_percentage'] * df_new['Guest_Popularity_percentage']\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# Apply feature engineering\n",
    "train_fe = engineer_features(train)\n",
    "test_fe = engineer_features(test)\n",
    "\n",
    "# Display new features\n",
    "new_features = [col for col in train_fe.columns if col not in train.columns]\n",
    "print(f\"New features created: {new_features}\")\n",
    "train_fe[new_features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7cd5ab",
   "metadata": {},
   "source": [
    "## 6. Missing Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36eaeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationship between missing values and target\n",
    "for col in ['Episode_Length_minutes', 'Guest_Popularity_percentage']:\n",
    "    train_fe[f'{col}_Missing'] = train_fe[col].isnull().astype(int)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=f'{col}_Missing', y='Listening_Time_minutes', data=train_fe)\n",
    "    plt.title(f'Listening Time by {col} Missing Status')\n",
    "    plt.xlabel(f'Is {col} Missing')\n",
    "    plt.ylabel('Listening Time (minutes)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print average listening time for missing vs non-missing\n",
    "    missing_avg = train_fe.groupby(f'{col}_Missing')['Listening_Time_minutes'].mean()\n",
    "    print(f\"Average Listening Time by {col} Missing Status:\")\n",
    "    print(missing_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47be79e3",
   "metadata": {},
   "source": [
    "## 7. Baseline Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a84a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "# Define features to use\n",
    "numerical_features = ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', \n",
    "                     'Number_of_Ads', 'Episode_Number', 'Day_Num', 'Time_Num', 'Is_Weekend', 'Sentiment_Score',\n",
    "                     'Host_Guest_Popularity_Diff', 'Host_Guest_Popularity_Product']\n",
    "\n",
    "categorical_features = ['Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment']\n",
    "\n",
    "# Add missing indicator features\n",
    "for col in ['Episode_Length_minutes', 'Guest_Popularity_percentage']:\n",
    "    train_fe[f'{col}_Missing'] = train_fe[col].isnull().astype(int)\n",
    "    test_fe[f'{col}_Missing'] = test_fe[col].isnull().astype(int)\n",
    "    numerical_features.append(f'{col}_Missing')\n",
    "\n",
    "# Split features and target\n",
    "X = train_fe[numerical_features + categorical_features]\n",
    "y = train_fe['Listening_Time_minutes']\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare the test data\n",
    "X_test = test_fe[numerical_features + categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f35ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "# Numerical preprocessing - impute missing values and scale\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical preprocessing - one-hot encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d110b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to evaluate different models\n",
    "def evaluate_model(model_name, model, X_train, X_val, y_train, y_val):\n",
    "    # Create pipeline with preprocessing and model\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    y_pred_val = pipeline.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    val_r2 = r2_score(y_val, y_pred_val)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{model_name} Results:\")\n",
    "    print(f\"Train RMSE: {train_rmse:.4f}\")\n",
    "    print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "    print(f\"Train R²: {train_r2:.4f}\")\n",
    "    print(f\"Validation R²: {val_r2:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    return pipeline, val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d1a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test several baseline models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    'LightGBM': lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "best_rmse = float('inf')\n",
    "best_model = None\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline, val_rmse = evaluate_model(name, model, X_train, X_val, y_train, y_val)\n",
    "    results[name] = val_rmse\n",
    "    \n",
    "    if val_rmse < best_rmse:\n",
    "        best_rmse = val_rmse\n",
    "        best_model = pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dac9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(list(results.keys()), list(results.values()))\n",
    "plt.xlabel('Validation RMSE (lower is better)')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce3eb6",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41842ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tree-based models, analyze feature importance\n",
    "# Train a simple model without the preprocessing pipeline to get feature names\n",
    "# Fill missing values for this analysis\n",
    "X_filled = X.copy()\n",
    "for col in numerical_features:\n",
    "    X_filled[col] = X_filled[col].fillna(X_filled[col].median())\n",
    "    \n",
    "# One-hot encode categorical features\n",
    "X_filled_encoded = pd.get_dummies(X_filled, columns=categorical_features, drop_first=False)\n",
    "\n",
    "# Train a random forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_filled_encoded, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X_filled_encoded.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances.head(20))\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28304f70",
   "metadata": {},
   "source": [
    "## 9. Make Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the best model\n",
    "test_predictions = best_model.predict(X_test)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_fe['id'],\n",
    "    'Listening_Time_minutes': test_predictions\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission.to_csv('../Datasets/model_submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created with predictions from the best model\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541573bf",
   "metadata": {},
   "source": [
    "## 10. Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've performed a comprehensive exploratory data analysis of the Podcast Listening Time Prediction dataset and built several baseline models. Here's a summary of our findings and potential next steps:\n",
    "\n",
    "### Key Findings:\n",
    "1. The dataset contains both categorical features (Genre, Publication Day/Time, Sentiment) and numerical features (Episode Length, Host/Guest Popularity).\n",
    "2. There are missing values in Episode Length and Guest Popularity that require handling.\n",
    "3. We engineered several potentially useful features, such as retention rate, day/time encodings, and popularity interaction features.\n",
    "4. Tree-based models (Random Forest, Gradient Boosting, XGBoost, LightGBM) generally performed better than linear models.\n",
    "\n",
    "### Next Steps:\n",
    "1. **Feature Engineering**: Create more advanced features like:\n",
    "   - Podcast-level aggregates (average listening time per podcast)\n",
    "   - Genre-specific features\n",
    "   - More interaction terms between features\n",
    "\n",
    "2. **Model Tuning**: Perform hyperparameter optimization for the best-performing models\n",
    "\n",
    "3. **Ensemble Methods**: Combine predictions from multiple models\n",
    "\n",
    "4. **Cross-Validation**: Implement k-fold cross-validation for more robust model evaluation\n",
    "\n",
    "5. **Original Dataset**: Consider incorporating the original Podcast Listening Time dataset as mentioned in the competition description\n",
    "\n",
    "6. **Advanced Models**: Try neural network approaches or more sophisticated algorithms\n",
    "\n",
    "7. **Missing Value Handling**: Explore more advanced imputation techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
